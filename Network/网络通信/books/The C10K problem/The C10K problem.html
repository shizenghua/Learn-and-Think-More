<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0030)http://www.kegel.com/c10k.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<title>The C10K problem</title>
</head>
<body>
<h1><a name="top">The C10K problem</a></h1>
<font color="red">[<a href="http://www.lwn.net/">Help save the best Linux news source on the web -- subscribe to Linux Weekly News!</a>]</font>
<p>
It's time for web servers to handle ten thousand clients simultaneously,
don't you think?  After all, the web is a big place now. 
</p><p>
And computers are big, too.  You can buy a 1000MHz machine
with 2 gigabytes of RAM and an 1000Mbit/sec Ethernet card for $1200 or so.  
Let's see - at 20000 clients, that's
50KHz, 100Kbytes, and 50Kbits/sec per client.  

It shouldn't take any more horsepower than that to take four kilobytes 
from the disk and send them to the network once a second for each
of twenty thousand clients.
(That works out to $0.08 per client, by the way.  Those
$100/client licensing fees some operating systems charge are starting to 
look a little heavy!)  So hardware is no longer the bottleneck.
</p><p>
In 1999 one of the busiest ftp sites, cdrom.com, 
actually handled 10000 clients simultaneously
through a Gigabit Ethernet pipe.
As of 2001, that same speed is now 
<a href="http://www.senteco.com/telecom/ethernet.htm">being offered by several ISPs</a>,
who expect it to become increasingly popular with large business customers.
</p><p>
And the thin client model of computing appears to be coming back in
style -- this time with the server out on the Internet, serving
thousands of clients.
</p><p>
With that in mind, here are a few notes on how to configure operating 
systems and write code to support thousands of clients.  The discussion
centers around Unix-like operating systems, as that's my personal area
of interest, but Windows is also covered a bit.

</p><h2>Contents</h2>
<ul>
<li><a href="http://www.kegel.com/c10k.html#top">The C10K problem</a>
</li><li><a href="http://www.kegel.com/c10k.html#related">Related Sites</a>
</li><li><a href="http://www.kegel.com/c10k.html#books">Book to Read First</a>
</li><li><a href="http://www.kegel.com/c10k.html#frameworks">I/O frameworks</a>
</li><li><a href="http://www.kegel.com/c10k.html#strategies">I/O Strategies</a>
<ol>
	<li><a href="http://www.kegel.com/c10k.html#nb">Serve many clients with each thread, and use nonblocking I/O and <b>level-triggered</b> readiness notification</a>
	<ul>
		<li><a href="http://www.kegel.com/c10k.html#nb.select">The traditional select()</a>
		</li><li><a href="http://www.kegel.com/c10k.html#nb.poll">The traditional poll()</a>
		</li><li><a href="http://www.kegel.com/c10k.html#nb./dev/poll">/dev/poll</a> (Solaris 2.7+)
		</li><li><a href="http://www.kegel.com/c10k.html#nb.kqueue">kqueue</a> (FreeBSD, NetBSD)
	</li></ul>
	</li><li><a href="http://www.kegel.com/c10k.html#nb.edge">Serve many clients with each thread, and use nonblocking I/O and readiness <b>change</b> notification</a>
	<ul>
		<li><a href="http://www.kegel.com/c10k.html#nb.epoll">epoll</a> (Linux 2.6+)
		</li><li><a href="http://www.kegel.com/c10k.html#nb.kevent">Polyakov's kevent</a> (Linux 2.6+)
		</li><li><a href="http://www.kegel.com/c10k.html#nb.newni">Drepper's New Network Interface</a> (proposal for Linux 2.6+)
		</li><li><a href="http://www.kegel.com/c10k.html#nb.sigio">Realtime Signals</a> (Linux 2.4+)
		</li><li><a href="http://www.kegel.com/c10k.html#nb.sigfd">Signal-per-fd</a>
		</li><li><a href="http://www.kegel.com/c10k.html#nb.kqueue">kqueue</a> (FreeBSD, NetBSD)
	</li></ul>
	</li><li><a href="http://www.kegel.com/c10k.html#aio">Serve many clients with each thread, and use asynchronous I/O and completion notification</a>
	</li><li><a href="http://www.kegel.com/c10k.html#threaded">Serve one client with each server thread</a>
	<ul>
		<li><a href="http://www.kegel.com/c10k.html#threads.linuxthreads">LinuxThreads</a> (Linux 2.0+)
		</li><li><a href="http://www.kegel.com/c10k.html#threads.ngpt">NGPT</a> (Linux 2.4+)
		</li><li><a href="http://www.kegel.com/c10k.html#threads.nptl">NPTL</a> (Linux 2.6, Red Hat 9)
		</li><li><a href="http://www.kegel.com/c10k.html#threads.freebsd">FreeBSD threading support</a>
		</li><li><a href="http://www.kegel.com/c10k.html#threads.netbsd">NetBSD threading support</a>
		</li><li><a href="http://www.kegel.com/c10k.html#threads.solaris">Solaris threading support</a>
        </li><li><a href="http://www.kegel.com/c10k.html#threads.java">Java threading support in JDK 1.3.x and earlier</a></li>
		<li><a href="http://www.kegel.com/c10k.html#1:1">Note: 1:1 threading vs. M:N threading</a>
	</li></ul>
	</li><li><a href="http://www.kegel.com/c10k.html#kio">Build the server code into the kernel</a>
	</li><li><a href="http://www.kegel.com/c10k.html#userspace">Bring the TCP stack into userspace</a>
</li></ol>
</li><li><a href="http://www.kegel.com/c10k.html#comments">Comments</a>
</li><li><a href="http://www.kegel.com/c10k.html#limits.filehandles">Limits on open filehandles</a>
</li><li><a href="http://www.kegel.com/c10k.html#limits.threads">Limits on threads</a>
</li><li><a href="http://www.kegel.com/c10k.html#java">Java issues</a> [Updated 27 May 2001]
</li><li><a href="http://www.kegel.com/c10k.html#tips">Other tips</a>
<ul>
	<li><a href="http://www.kegel.com/c10k.html#zerocopy">Zero-Copy</a>
	</li><li><a href="http://www.kegel.com/c10k.html#sendfile">The sendfile() system call can implement zero-copy networking.</a>
	</li><li><a href="http://www.kegel.com/c10k.html#writev">Avoid small frames by using writev (or TCP_CORK)</a>
	</li><li><a href="http://www.kegel.com/c10k.html#nativethreads">Some programs can benefit from using non-Posix threads.</a>
	</li><li><a href="http://www.kegel.com/c10k.html#caching">Caching your own data can sometimes be a win.</a>
</li></ul>
</li><li><a href="http://www.kegel.com/c10k.html#limits.other">Other limits</a>
</li><li><a href="http://www.kegel.com/c10k.html#kernel">Kernel Issues</a> 
</li><li><a href="http://www.kegel.com/c10k.html#benchmarking">Measuring Server Performance</a>
</li><li><a href="http://www.kegel.com/c10k.html#examples">Examples</a>
<ul>
	<li><a href="http://www.kegel.com/c10k.html#examples.nb.select">Interesting select()-based servers</a>
	</li><li><a href="http://www.kegel.com/c10k.html#examples.nb./dev/poll">Interesting /dev/poll-based servers</a>
	</li><li><a href="http://www.kegel.com/c10k.html#examples.nb.epoll">Interesting epoll-based servers</a>
	</li><li><a href="http://www.kegel.com/c10k.html#examples.nb.kqueue">Interesting kqueue()-based servers</a>
	</li><li><a href="http://www.kegel.com/c10k.html#examples.nb.sigio">Interesting realtime signal-based servers</a> 
	</li><li><a href="http://www.kegel.com/c10k.html#examples.threaded">Interesting thread-based servers</a>
	</li><li><a href="http://www.kegel.com/c10k.html#examples.kio">Interesting in-kernel servers</a>
</li></ul>
</li><li><a href="http://www.kegel.com/c10k.html#links">Other interesting links</a>
</li></ul>
<p>

</p><h2><a name="related">Related Sites</a></h2>
See Nick Black's execellent <a href="http://dank.qemfd.net/dankwiki/index.php/Network_servers">Fast UNIX Servers</a>
page
for a circa-2009 look at the situation.
<p>
In October 2003, Felix von Leitner put together an excellent <a href="http://bulk.fefe.de/scalability/">web page</a>
and <a href="http://bulk.fefe.de/scalable-networking.pdf">presentation</a> about network scalability,
complete with benchmarks comparing various networking system calls and operating systems.
One of his observations is that the 2.6 Linux kernel really does beat the 2.4 kernel,
but there are many, many good graphs that will give the OS developers food for thought for some time.
(See also the <a href="http://developers.slashdot.org/developers/03/10/19/0130256.shtml?tid=106&amp;tid=130&amp;tid=185&amp;tid=190">Slashdot</a>
comments; it'll be interesting to see whether anyone does followup benchmarks improving on Felix's results.)

</p><h2><a name="books">Book to Read First</a></h2>
<p>
If you haven't read it already, go out and get a copy of
<a href="http://www.amazon.com/exec/obidos/ASIN/013490012X/">
Unix Network Programming : Networking Apis: Sockets and Xti (Volume 1)</a>
by the late W. Richard Stevens.  It describes many of the I/O
strategies and pitfalls related to writing high-performance servers.
It even talks about the <a href="http://www.citi.umich.edu/projects/linux-scalability/reports/accept.html">'thundering herd'</a> problem.
And while you're at it, go read <a href="http://pl.atyp.us/content/tech/servers.html">Jeff Darcy's notes on high-performance server design</a>.
</p><p>
(Another book which might be more helpful for those
who are *using* rather than *writing* a web server is
<a href="http://www.amazon.com/gp/product/0596102356">Building Scalable Web Sites</a> by Cal Henderson.)

</p><h3><a name="frameworks">I/O frameworks</a></h3>
<p>
Prepackaged libraries are available that abstract some of the techniques presented below, 
insulating your code from the operating system and making it more portable.
</p><ul>
<li><a href="http://www.cs.wustl.edu/~schmidt/ACE.html">ACE</a>, a heavyweight C++ I/O framework,
contains object-oriented implementations of some of these I/O strategies
and many other useful things.
In particular, his Reactor is an OO way of doing nonblocking I/O, and
Proactor is an OO way of doing asynchronous I/O.
</li><li><a href="http://asio.sf.net/">ASIO</a> is an C++ I/O framework
which is becoming part of the Boost library.  It's like ACE updated for 
the STL era.
</li><li>
<a href="http://monkey.org/~provos/libevent">libevent</a> is a lightweight C
I/O framework by Niels Provos.  It supports kqueue and select,
and soon will support poll and epoll.  It's level-triggered only, I think,
which has both good and bad sides.  Niels has 
<a href="http://monkey.org/~provos/libevent/libevent-benchmark.jpg">a nice graph of time to handle one event</a>
 as a function of the number of connections.  It shows kqueue and sys_epoll
as clear winners.
</li><li>My own attempts at lightweight frameworks (sadly, not kept up to date):
<ul>
<li>
<a href="http://www.kegel.com/dkftpbench/Poller_bench.html">Poller</a> is a lightweight C++ 
I/O framework that implements a level-triggered readiness API using whatever underlying 
readiness API you want (poll, select, /dev/poll, kqueue, or sigio).
It's useful for <a href="http://www.kegel.com/dkftpbench/Poller_bench.html">benchmarks that compare
the performance of the various APIs.</a>  This document links to 
Poller subclasses below to illustrate how each of the readiness APIs
can be used.
</li><li>
<a href="http://www.kegel.com/rn/">rn</a> is a lightweight C I/O framework that was my second try
after Poller.  It's lgpl (so it's easier to use in commercial apps) and
C (so it's easier to use in non-C++ apps).  It was used in some commercial
products.
</li></ul>
</li><li>
Matt Welsh wrote <a href="http://www.cs.berkeley.edu/~mdw/papers/events.pdf">a paper</a>
in April 2000 about how to balance the use of worker thread and
event-driven techniques when building scalable servers.  
The paper describes part of his Sandstorm I/O framework.
</li><li>
<a href="http://svn.sourceforge.net/viewcvs.cgi/*checkout*/int64/scale/readme.txt">Cory Nelson's Scale! library</a> - an async socket, file, and pipe I/O library for Windows
</li></ul>

<h2><a name="strategies">I/O Strategies</a></h2>
Designers of networking software have many options.  Here are a few:
<ul>
<li>Whether and how to issue multiple I/O calls from a single thread
<ul>
	<li>Don't; use blocking/synchronous calls throughout, and possibly use multiple threads or processes to achieve concurrency
	</li><li>Use nonblocking calls (e.g. write() on a socket set to O_NONBLOCK) to start I/O,
	and readiness notification (e.g. poll() or /dev/poll) to know when it's OK to start the next I/O on that channel.
	Generally only usable with network I/O, not disk I/O.
	</li><li>Use asynchronous calls (e.g. aio_write()) to start I/O, and completion notification (e.g. signals or completion ports)
	to know when the I/O finishes.  Good for both network and disk I/O.
</li></ul>
</li><li>How to control the code servicing each client
<ul>
	<li>one process for each client (classic Unix approach, used since 1980 or so)
	</li><li>one OS-level thread handles many clients; each client is controlled by:
	<ul>
		<li>a user-level thread (e.g. GNU state threads, classic Java with green threads)
		</li><li>a state machine (a bit esoteric, but popular in some circles; my favorite)
		</li><li>a continuation (a bit esoteric, but popular in some circles)
	</li></ul>
	</li><li>one OS-level thread for each client (e.g. classic Java with native threads)
	</li><li>one OS-level thread for each active client (e.g. Tomcat with apache front end; NT completion ports; thread pools)
</li></ul>
</li><li>Whether to use standard O/S services, or put some code into the 
kernel (e.g. in a custom driver, kernel module, or VxD)
</li></ul>
<p>
The following five combinations seem to be popular:
</p><ol>
<li><a href="http://www.kegel.com/c10k.html#nb">Serve many clients with each thread, and use nonblocking I/O and <b>level-triggered</b> readiness notification</a>
</li><li><a href="http://www.kegel.com/c10k.html#nb.edge">Serve many clients with each thread, and use nonblocking I/O and readiness <b>change</b> notification</a>
</li><li><a href="http://www.kegel.com/c10k.html#aio">Serve many clients with each server thread, and use asynchronous I/O</a>
</li><li><a href="http://www.kegel.com/c10k.html#threaded">serve one client with each server thread, and use blocking I/O</a>
</li><li><a href="http://www.kegel.com/c10k.html#kio">Build the server code into the kernel</a>
</li></ol>

<h3><a name="nb">1. Serve many clients with each thread, and use nonblocking I/O and <b>level-triggered</b> readiness notification</a></h3>
<p>
... set nonblocking mode on all network handles, and use
select() or poll() to tell which network handle has data waiting.
This is the traditional favorite.
With this scheme, the kernel tells you whether a file descriptor is ready,
whether or not you've done anything with that file descriptor since the last time
the kernel told you about it.  (The name 'level triggered' comes from computer hardware
design; it's the opposite of <a href="http://www.kegel.com/c10k.html#nb.edge">'edge triggered'</a>.
Jonathon Lemon introduced the terms in his 
<a href="http://people.freebsd.org/~jlemon/papers/kqueue.pdf">BSDCON 2000 paper on kqueue()</a>.)
</p><p>
Note: it's particularly important to remember that readiness notification from the
kernel is only a hint; the file descriptor might not be ready anymore when you try
to read from it.  That's why it's important to use nonblocking mode when using
readiness notification.
</p><p>
An important bottleneck in this method is that read() or sendfile() 
from disk blocks if the page is not in core at the moment;
setting nonblocking mode on a disk file handle has no effect.
Same thing goes for memory-mapped disk files.
The first time a server needs disk I/O, its process blocks,
all clients must wait, and that raw nonthreaded performance goes to waste.
<br>
This is what asynchronous I/O is for, but on systems that lack AIO,
worker threads or processes that do the disk I/O can also get around this
bottleneck.  One approach is to use memory-mapped files,
and if mincore() indicates I/O is needed, ask a worker to do the I/O,
and continue handling network traffic.  Jef Poskanzer mentions that
Pai, Druschel, and Zwaenepoel's 1999 <a href="http://www.cs.rice.edu/~vivek/flash99/">Flash</a> web server uses this trick; they gave a talk at 
<a href="http://www.usenix.org/events/usenix99/technical.html">Usenix '99</a> on it.
It looks like mincore() is available in BSD-derived Unixes 
like <a href="http://www.freebsd.org/cgi/man.cgi?query=mincore">FreeBSD</a>
and Solaris, but is not part
of the <a href="http://www.unix-systems.org/">Single Unix Specification</a>.
It's available as part of Linux as of kernel 2.3.51, 
<a href="http://www.citi.umich.edu/projects/citi-netscape/status/mar-apr2000.html">thanks to Chuck Lever</a>.
</p><p>
But <a href="http://marc.theaimsgroup.com/?l=freebsd-hackers&amp;m=106718343317930&amp;w=2">
in November 2003 on the freebsd-hackers list, Vivek Pei et al reported</a> 
very good results using system-wide profiling of their Flash web server
to attack bottlenecks.  One bottleneck they found was
mincore (guess that wasn't such a good idea after all)
Another was the fact that sendfile blocks on disk access;
they improved performance by introducing a modified sendfile() 
that return something like EWOULDBLOCK
when the disk page it's fetching is not yet in core.
(Not sure how you tell the user the page is now resident...
seems to me what's really needed here is aio_sendfile().)
The end result of their optimizations is a SpecWeb99 score of about 800
on a 1GHZ/1GB FreeBSD box, which is better than anything on
file at spec.org.
</p><p>
There are several ways for a single thread to tell which of a set of nonblocking sockets are ready for I/O:
</p><ul>
<li><a name="nb.select"><b>The traditional select()</b></a> <br>
Unfortunately, select() is limited to FD_SETSIZE handles.  
This limit is compiled in to the standard library and user programs.
(Some versions of the C library let you raise this limit at user app compile time.)
<p>
See 
<a href="http://www.kegel.com/dkftpbench/doc/Poller_select.html">Poller_select</a>
(<a href="http://www.kegel.com/dkftpbench/dkftpbench-0.44/Poller_select.cc">cc</a>, 
<a href="http://www.kegel.com/dkftpbench/dkftpbench-0.44/Poller_select.h">h</a>)
for an example of how to use select() interchangeably with other readiness notification schemes.
</p><p>
</p></li><li><a name="nb.poll"><b>The traditional poll()</b></a> <br>
There is no hardcoded limit to the number of file descriptors poll() can handle,
but it does get slow about a few thousand, since most of the file descriptors
are idle at any one time, and scanning through thousands of file descriptors
takes time.
<p>
Some OS's (e.g. Solaris 8) speed up poll() et al by use of techniques like poll hinting,
which was
<a href="http://www.humanfactor.com/cgi-bin/cgi-delegate/apache-ML/nh/1999/May/0415.html">implemented and benchmarked by Niels Provos</a> for Linux in 1999.
</p><p>
See 
<a href="http://www.kegel.com/dkftpbench/doc/Poller_poll.html">Poller_poll</a>
(<a href="http://www.kegel.com/dkftpbench/dkftpbench-0.44/Poller_poll.cc">cc</a>, 
<a href="http://www.kegel.com/dkftpbench/dkftpbench-0.44/Poller_poll.h">h</a>,
<a href="http://www.kegel.com/dkftpbench/Poller_bench.html">benchmarks</a>)
for an example of how to use poll() interchangeably with other readiness notification schemes.
</p><p>
</p></li><li><a name="nb./dev/poll"><b>/dev/poll</b></a><br>
This is the recommended poll replacement for Solaris.
<p>
The idea behind /dev/poll is to take advantage of the fact that often
poll() is called many times with the same arguments.
With /dev/poll, you get an open handle to /dev/poll, and
tell the OS just once what files you're interested in by writing to that handle;
from then on, you just read the set of currently ready file descriptors from that handle.
</p><p>
It appeared quietly in Solaris 7
(<a href="http://sunsolve.sun.com/pub-cgi/retrieve.pl?patchid=106541&amp;collection=fpatches">see patchid 106541</a>)
but its first public appearance was in 
<a href="http://docs.sun.com/ab2/coll.40.6/REFMAN7/@Ab2PageView/55123?Ab2Lang=C&amp;Ab2Enc=iso-8859-1">Solaris 8</a>; 
<a href="http://www.sun.com/sysadmin/ea/poll.html">according to Sun</a>,
at 750 clients, this has 10% of the overhead of poll().
</p><p>
Various implementations of /dev/poll were tried on Linux, but 
none of them perform as well as epoll, and were never really completed.
/dev/poll use on Linux is not recommended.
</p><p>
See 
<a href="http://www.kegel.com/dkftpbench/doc/Poller_devpoll.html">Poller_devpoll</a>
(<a href="http://www.kegel.com/dkftpbench/dkftpbench-0.44/Poller_devpoll.cc">cc</a>, 
<a href="http://www.kegel.com/dkftpbench/dkftpbench-0.44/Poller_devpoll.h">h</a>
<a href="http://www.kegel.com/dkftpbench/Poller_bench.html">benchmarks</a>
)
for an example of how to use /dev/poll interchangeably with many other readiness notification 
schemes.  (Caution - the example is for Linux /dev/poll, might not work right on Solaris.)
</p><p>
</p></li><li><b>kqueue()</b><br>
This is the recommended poll replacement for FreeBSD (and, soon, NetBSD).
<p>
<a href="http://www.kegel.com/c10k.html#nb.kqueue">See below.</a>  kqueue() can specify either edge triggering or level triggering.
</p></li></ul>

<h3><a name="nb.edge">2. Serve many clients with each thread, and use nonblocking I/O and readiness <b>change</b> notification</a></h3>
Readiness change notification (or edge-triggered readiness notification)
means you give the kernel a file descriptor, and later, when that descriptor transitions from 
<i>not ready</i> to <i>ready</i>, the kernel notifies you somehow.  It then assumes you
know the file descriptor is ready, and will not send any more readiness
notifications of that type for that file descriptor until you do something
that causes the file descriptor to no longer be ready (e.g. until you receive the
EWOULDBLOCK error on a send, recv, or accept call, or a send or recv transfers
less than the requested number of bytes).  
<p>
When you use readiness change notification, you must be prepared for spurious
events, since one common implementation is to signal readiness whenever any
packets are received, regardless of whether the file descriptor was already ready.
</p><p>
This is the opposite of "<a href="http://www.kegel.com/c10k.html#nb">level-triggered</a>" readiness notification.
It's a bit less forgiving of programming mistakes, since
if you miss just one event, the connection that event was for gets stuck forever.
Nevertheless, I have found that edge-triggered readiness notification
made programming nonblocking clients with OpenSSL easier, so it's worth trying.
</p><p>
<a href="http://www.cs.rice.edu/~druschel/usenix99event.ps.gz">[Banga, Mogul, Drusha '99]</a>
described this kind of scheme in 1999.
</p><p>
There are several APIs which let the application retrieve 'file descriptor became ready' notifications:
</p><ul>
<li><a name="nb.kqueue"><b>kqueue()</b></a>
This is the recommended edge-triggered poll replacement for FreeBSD (and, soon, NetBSD).
<p>
FreeBSD 4.3 and later, and <a href="http://kerneltrap.org/node.php?id=472">NetBSD-current as of Oct 2002</a>, 
support a generalized alternative to poll() called
<a href="http://www.freebsd.org/cgi/man.cgi?query=kqueue&amp;apropos=0&amp;sektion=0&amp;manpath=FreeBSD+5.0-current&amp;format=html">kqueue()/kevent()</a>; 
it supports both edge-triggering and level-triggering.
(See also <a href="http://people.freebsd.org/~jlemon/">Jonathan Lemon's page</a>
and his <a href="http://people.freebsd.org/~jlemon/papers/kqueue.pdf">BSDCon 2000 paper on kqueue()</a>.)
</p><p>
Like /dev/poll, you allocate a listening object, but rather than opening the file /dev/poll, you
call kqueue() to allocate one.  To change the events you are listening for, or to get the
list of current events, you call kevent() on the descriptor returned by kqueue().
It can listen not just for socket readiness, but also for plain file readiness, signals, and even for I/O completion.
</p><p>
<b>Note:</b> as of October 2000, the threading library on FreeBSD does not interact well with kqueue();
evidently, when kqueue() blocks, the entire process blocks, not just the calling thread.
</p><p>
See
<a href="http://www.kegel.com/dkftpbench/doc/Poller_kqueue.html">Poller_kqueue</a>
(<a href="http://www.kegel.com/dkftpbench/dkftpbench-0.44/Poller_kqueue.cc">cc</a>, 
<a href="http://www.kegel.com/dkftpbench/dkftpbench-0.44/Poller_kqueue.h">h</a>,
<a href="http://www.kegel.com/dkftpbench/Poller_bench.html">benchmarks</a>)
for an example of how to use kqueue() interchangeably with
many other readiness notification schemes.
</p><p>
Examples and libraries using kqueue():
</p><ul>
<li><a href="http://people.freebsd.org/~dwhite/PyKQueue/">PyKQueue</a> -- a Python binding for kqueue()
</li><li><a href="http://www.monkeys.com/kqueue/echo.c">Ronald F. Guilmette's example echo server</a>; see also 
<a href="http://groups.yahoo.com/group/freebsd-questions/message/223580">his 28 Sept 2000 post on freebsd.questions</a>.
</li></ul>
<p>

</p></li><li><a name="nb.epoll"><b>epoll</b></a><br>
This is the recommended edge-triggered poll replacement for the 2.6 Linux kernel.
<p>
On 11 July 2001, Davide Libenzi proposed an alternative to realtime signals; his
patch provides what he now calls
<a href="http://www.xmailserver.org/linux-patches/nio-improve.html">/dev/epoll
www.xmailserver.org/linux-patches/nio-improve.html</a>.  This is just like
the realtime signal readiness notification, but it coalesces redundant events,
and has a more efficient scheme for bulk event retrieval.  
</p><p>
Epoll was merged into the 2.5 kernel tree as of 2.5.46
after its interface was changed from a special file in /dev
to a system call, sys_epoll.  A patch for the older version of epoll is available for the 2.4 kernel.
</p><p>
There was a lengthy debate about 
<a href="http://marc.theaimsgroup.com/?l=linux-kernel&amp;m=103607925020720&amp;w=2">unifying epoll, aio,
and other event sources</a> on the linux-kernel mailing list around Halloween 2002.
It may yet happen, but Davide is concentrating on firming up epoll in general first.
</p><p>
</p></li><li><a name="nb.kevent">Polyakov's kevent</a> (Linux 2.6+)
<a name="kevent">
News flash: On 9 Feb 2006, and again on 9 July 2006, Evgeniy Polyakov posted patches which
seem to unify epoll and aio; his goal is to support network AIO.</a>  See:
<ul>
<li><a href="http://lwn.net/Articles/172844/">the LWN article about kevent</a>
</li><li><a href="http://lkml.org/lkml/2006/7/9/82">his July announcement</a>
</li><li><a href="http://tservice.net.ru/~s0mbre/old/?section=projects&amp;item=kevent">his kevent page</a>
</li><li><a href="http://tservice.net.ru/~s0mbre/old/?section=projects&amp;item=naio">his naio page</a>
</li><li><a href="http://thread.gmane.org/gmane.linux.network/37595/focus=37673">some recent discussion</a>
</li></ul>
<p>
</p></li><li><a name="nb.newni">Drepper's New Network Interface</a> (proposal for Linux 2.6+)<br>
At OLS 2006, Ulrich Drepper proposed a new high-speed asynchronous networking API.
See:
<ul>
<li>his paper, "<a href="http://people.redhat.com/drepper/newni.pdf">The Need for Asynchronous, Zero-Copy Network I/O</a>"
</li><li><a href="http://people.redhat.com/drepper/newni-slides.pdf">his slides</a>
</li><li><a href="http://lwn.net/Articles/192410/">LWN article from July 22</a>
</li></ul>
<p>
</p></li><li><a name="nb.sigio"><b>Realtime Signals</b></a><br>
This is the recommended edge-triggered poll replacement for the 2.4 Linux kernel.
<p>
The 2.4 linux kernel can deliver socket readiness events via a particular realtime signal.
Here's how to turn this behavior on:
</p><pre>/* Mask off SIGIO and the signal you want to use. */
sigemptyset(&amp;sigset);
sigaddset(&amp;sigset, signum);
sigaddset(&amp;sigset, SIGIO);
sigprocmask(SIG_BLOCK, &amp;m_sigset, NULL);
/* For each file descriptor, invoke F_SETOWN, F_SETSIG, and set O_ASYNC. */
fcntl(fd, F_SETOWN, (int) getpid());
fcntl(fd, F_SETSIG, signum);
flags = fcntl(fd, F_GETFL);
flags |= O_NONBLOCK|O_ASYNC;
fcntl(fd, F_SETFL, flags);
</pre>
This sends that signal when a normal I/O function like read() or write() completes.
To use this, write a normal poll() outer loop, and inside it, after you've handled all 
the fd's noticed by poll(), you loop calling 
<a href="http://www.opengroup.org/onlinepubs/007908799/xsh/sigwaitinfo.html">sigwaitinfo()</a>.<br>
If sigwaitinfo or sigtimedwait returns your realtime signal, siginfo.si_fd and 
siginfo.si_band give almost the same information as pollfd.fd and pollfd.revents would 
after a call to poll(), so you handle the i/o, and continue calling sigwaitinfo().<br>
If sigwaitinfo returns a traditional SIGIO, the signal queue overflowed,
so you 
<a href="http://www.cs.helsinki.fi/linux/linux-kernel/Year-1999/1999-41/0644.html">flush the signal queue by temporarily changing the signal handler to SIG_DFL</a>,
and break back to the outer poll() loop.  <br>
<p>
See
<a href="http://www.kegel.com/dkftpbench/doc/Poller_sigio.html">Poller_sigio</a>
(<a href="http://www.kegel.com/dkftpbench/dkftpbench-0.44/Poller_sigio.cc">cc</a>, 
<a href="http://www.kegel.com/dkftpbench/dkftpbench-0.44/Poller_sigio.h">h</a>)
for an example of how to use rtsignals interchangeably with 
many other readiness notification schemes.
</p><p>
See <a href="http://www.kegel.com/c10k.html#phhttpd">Zach Brown's phhttpd</a> for example code that uses this feature
directly.  (Or don't; phhttpd is a bit hard to figure out...)
</p><p>
[<a href="http://www.citi.umich.edu/techreports/reports/citi-tr-00-7.ps.gz">Provos, Lever, and Tweedie 2000</a>] describes
a recent benchmark of phhttpd using a variant of sigtimedwait(), sigtimedwait4(), that lets you
retrieve multiple signals with one call.  Interestingly, the chief benefit
of sigtimedwait4() for them seemed to be it allowed the app to gauge system 
overload (so it could <a href="http://www.kegel.com/c10k.html#overload">behave appropriately</a>).
(Note that poll() provides the same measure of system overload.)
</p><p>
</p></li><li><a name="nb.sigfd"><b>Signal-per-fd</b></a><br>
Chandra and Mosberger proposed a modification to the realtime signal approach called
"signal-per-fd" which reduces or eliminates realtime signal queue overflow
by coalescing redundant events.  It doesn't outperform epoll, though.
Their paper (
<a href="http://www.hpl.hp.com/techreports/2000/HPL-2000-174.html">www.hpl.hp.com/techreports/2000/HPL-2000-174.html</a>)
compares performance of this scheme with select() and /dev/poll.<br>
<p>
<a href="http://boudicca.tux.org/hypermail/linux-kernel/2001week20/1353.html">
Vitaly Luban announced a patch implementing this scheme on 18 May 2001</a>; his
patch lives at <a href="http://www.luban.org/GPL/gpl.html">www.luban.org/GPL/gpl.html</a>.
(Note: as of Sept 2001, there may still be stability problems with this patch under heavy load.
<a href="http://www.kegel.com/dkftpbench">dkftpbench</a> at about 4500 users may be able to trigger an oops.)
</p><p>
See
<a href="http://www.kegel.com/dkftpbench/doc/Poller_sigfd.html">Poller_sigfd</a>
(<a href="http://www.kegel.com/dkftpbench/dkftpbench-0.44/Poller_sigfd.cc">cc</a>, 
<a href="http://www.kegel.com/dkftpbench/dkftpbench-0.44/Poller_sigfd.h">h</a>)
for an example of how to use signal-per-fd interchangeably with 
many other readiness notification schemes.
</p></li></ul>

<h3><a name="aio">3. Serve many clients with each server thread, and use asynchronous I/O</a></h3>
<p>
This has not yet become popular in Unix,
probably because few operating systems support asynchronous I/O,
also possibly because it (like nonblocking I/O) requires rethinking your application.
Under standard Unix, asynchronous I/O is provided by <a href="http://www.opengroup.org/onlinepubs/007908799/xsh/realtime.html">the aio_ interface</a>
(scroll down from that link to "Asynchronous input and output"), 
which associates a signal and value with each I/O operation.
Signals and their values are queued and delivered efficiently to the user process.
This is from the POSIX 1003.1b realtime extensions, and is also in the Single Unix Specification,
version 2.  
</p><p>
AIO is normally used with edge-triggered completion notification, i.e. a
signal is queued when the operation is complete.  (It can also be used
with level triggered completion notification by calling
<a href="http://www.opengroup.org/onlinepubs/007908799/xsh/aio_suspend.html">aio_suspend()</a>,
though I suspect few people do this.)
</p><p>
glibc 2.1 and later provide a generic implementation
written for standards compliance rather than performance.
</p><p>
Ben LaHaise's implementation for Linux AIO was merged into the
main Linux kernel as of 2.5.32.  It doesn't use kernel threads, and 
has a very efficient underlying api, but (as of 2.6.0-test2) doesn't yet 
support sockets.  (There is also an AIO patch for the 2.4 kernels,
but the 2.5/2.6 implementation is somewhat different.)  More info:
</p><ul>
<li>The page "<a href="http://lse.sourceforge.net/io/aio.html">Kernel Asynchronous I/O (AIO) Support for Linux</a>" which tries to tie together all info about the 2.6 kernel's implementation of AIO (posted 16 Sept 2003)
</li><li><a href="http://www.linuxsymposium.org/2002/view_txt.php?text=abstract&amp;talk=11">Round 3: aio vs /dev/epoll</a> by Benjamin C.R. LaHaise (presented at 2002 OLS)
</li><li><a href="http://archive.linuxsymposium.org/ols2003/Proceedings/All-Reprints/Reprint-Pulavarty-OLS2003.pdf">Asynchronous I/O Suport in Linux 2.5</a>, by Bhattacharya, Pratt, Pulaverty, and Morgan, IBM; presented at OLS '2003
</li><li><a href="http://sourceforge.net/docman/display_doc.php?docid=12548&amp;group_id=8875">Design Notes on Asynchronous I/O (aio) for Linux</a>
by Suparna Bhattacharya --
compares Ben's AIO with SGI's KAIO and a few other AIO projects
</li><li><a href="http://www.kvack.org/~blah/aio/">Linux AIO home page</a> - Ben's preliminary patches, mailing list, etc.
</li><li><a href="http://marc.theaimsgroup.com/?l=linux-aio">linux-aio mailing list archives</a>
</li><li><a href="http://www.ocfs.org/aio/">libaio-oracle</a> - library implementing standard Posix AIO on top of libaio.  
<a href="http://marc.theaimsgroup.com/?l=linux-aio&amp;m=105069158425822&amp;w=2">First mentioned by Joel Becker on 18 Apr 2003</a>.
</li></ul>
Suparna also suggests having a look at the
<a href="http://www.dafscollaborative.org/tools/dafs_api.pdf">the DAFS API's approach to AIO</a>.
<p>
<a href="http://www.ussg.iu.edu/hypermail/linux/kernel/0209.0/0832.html">Red Hat AS</a>
and Suse SLES both provide a high-performance implementation on the 2.4 kernel;
it is related to, but not completely identical to, the 2.6 kernel implementation.
</p><p>
In February 2006, a new attempt is being made to provide network AIO; see <a href="http://www.kegel.com/c10k.html#kevent">the note above about Evgeniy Polyakov's kevent-based AIO</a>.
</p><p>
In 1999, <b><a href="http://oss.sgi.com/projects/kaio/">SGI implemented high-speed AIO</a> for Linux</b>.  As of version 1.1, it's said to work well with both
disk I/O and sockets.  It seems to use kernel threads.
It is still useful for people who can't wait for Ben's AIO to support sockets.
</p><p>
The O'Reilly book 
<a href="http://www.oreilly.com/catalog/posix4/">POSIX.4: Programming for the Real World</a>
is said to include a good introduction to aio.
</p><p>
A tutorial for the earlier, nonstandard, aio implementation on Solaris
is online at 
<a href="http://sunsite.nstu.nsk.su/sunworldonline/swol-03-1996/swol-03-aio.html">Sunsite</a>.
It's probably worth a look, but keep in mind you'll need to mentally
convert "aioread" to "aio_read", etc.
</p><p>
Note that AIO doesn't provide a way to open files without blocking for disk I/O;
if you care about the sleep caused by opening a disk file, 
<a href="http://www.ussg.iu.edu/hypermail/linux/kernel/0102.1/0124.html">Linus suggests</a> 
you should simply do the open() in a different thread rather
than wishing for an aio_open() system call.
</p><p>
Under Windows, asynchronous I/O is associated with the terms
"Overlapped I/O" and IOCP or "I/O Completion Port".  
Microsoft's IOCP combines techniques from the
prior art like asynchronous I/O (like aio_write) and queued completion
notification (like when using the aio_sigevent field with aio_write)
with a new idea of holding back some requests to try to keep the number
of running threads associated with a single IOCP constant.
For more information, see 
<a href="http://www.sysinternals.com/ntw2k/info/comport.shtml">Inside I/O Completion Ports</a>
by Mark Russinovich at sysinternals.com, Jeffrey Richter's
book "Programming Server-Side Applications for Microsoft Windows 2000"
(<a href="http://www.amazon.com/exec/obidos/ASIN/0735607532">Amazon</a>,
<a href="http://www.microsoft.com/mspress/books/toc/3402.asp">MSPress</a>),
<a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PALL&amp;p=1&amp;u=/netahtml/srchnum.htm&amp;r=1&amp;f=G&amp;l=50&amp;s1=%276223207%27.WKU.&amp;OS=PN/6223207&amp;RS=PN/6223207">U.S. patent #06223207</a>, or 
<a href="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/fileio/filesio_4z1v.asp">MSDN</a>.


</p><h3><a name="threaded">4. Serve one client with each server thread</a></h3>
<p>
... and let read() and write() block.  Has the disadvantage of using a whole stack
frame for each client, which costs memory.  Many OS's also have trouble handling more
than a few hundred threads.  If each thread gets a 2MB stack (not an uncommon
default value), you run out of *virtual memory* at (2^30 / 2^21) = 512 threads 
on a 32 bit machine with 1GB user-accessible VM (like, say, Linux as normally shipped on x86).
You can work around this by giving each thread a smaller stack,
but since most thread libraries don't allow growing thread stacks
once created, doing this means designing your program to minimize
stack use.  You can also work around this by moving to a 64 bit processor.
</p><p>
The thread support in Linux, FreeBSD, and Solaris is improving,
and 64 bit processors are just around the corner even for mainstream users.
Perhaps in the not-too-distant future, those who prefer using
one thread per client will be able to use that paradigm even
for 10000 clients.
Nevertheless, at the current time, if you actually want to support that many clients,
you're probably better off using some other paradigm.
</p><p>
For an unabashedly pro-thread viewpoint, see 
<a href="http://www.usenix.org/events/hotos03/tech/vonbehren.html">Why Events Are A Bad Idea (for High-concurrency Servers)</a> 
by von Behren, Condit, and Brewer, UCB, presented at HotOS IX.  
Anyone from the anti-thread camp care to point out a paper that rebuts this one?  :-)

</p><h4><a name="threads.linuxthreads">LinuxThreads</a></h4>
<a href="http://pauillac.inria.fr/~xleroy/linuxthreads/">LinuxTheads</a> is the
name for the standard Linux thread library.  It is integrated into glibc since
glibc2.0, and is mostly Posix-compliant, but with less than stellar performance
and signal support.

<h4><a name="threads.ngpt">NGPT: Next Generation Posix Threads for Linux</a></h4>
<a href="http://www-124.ibm.com/pthreads/">NGPT</a> is a project
started by IBM to bring good Posix-compliant thread support to Linux.  It's
at stable version 2.2 now, and works well... but the NGPT team has
<a href="http://www-124.ibm.com/pthreads/docs/announcement">announced</a>
that they are putting the NGPT codebase into support-only mode 
because they feel it's "the best way to support the community
for the long term".  The NGPT team will continue working to improve
Linux thread support, but now focused on improving NPTL.
(Kudos to the NGPT team for their good work and the graceful way they
conceded to NPTL.)  

<h4><a name="threads.nptl">NPTL: Native Posix Thread Library for Linux</a></h4>
<a href="http://people.redhat.com/drepper/nptl/">NPTL</a> is a project by 
<a href="http://people.redhat.com/drepper/">Ulrich Drepper</a>
(the benevolent dict^H^H^H^Hmaintainer of 
<a href="http://www.gnu.org/software/libc/">glibc</a>) and 
<a href="http://people.redhat.com/mingo/">Ingo Molnar</a>
to bring world-class Posix threading support to Linux.
<p>
As of 5 October 2003, NPTL is now merged into the glibc cvs tree as an add-on
directory (just like linuxthreads), so it will almost certainly be released
along with the next release of glibc.
</p><p>
The first major distribution to include an early snapshot of NPTL was Red Hat 9.
(This was a bit inconvenient for some users, but somebody had to break the ice...)
</p><p>
NPTL links:
</p><ul>
<li><a href="https://listman.redhat.com/mailman/listinfo/phil-list">Mailing list for NPTL discussion</a> 
</li><li><a href="http://people.redhat.com/drepper/nptl/">NPTL source code</a>
</li><li><a href="http://lwn.net/Articles/10465/">Initial announcement for NPTL</a>
</li><li><a href="http://people.redhat.com/drepper/glibcthreads.html">Original whitepaper describing the goals for NPTL</a>
</li><li><a href="http://people.redhat.com/drepper/nptl-design.pdf">Revised whitepaper describing the final design of NPTL</a>
</li><li><a href="http://marc.theaimsgroup.com/?l=linux-kernel&amp;m=103230439008204&amp;w=2">Ingo Molnar's</a> first benchmark showing it could handle 10^6 threads
</li><li><a href="http://marc.theaimsgroup.com/?l=linux-kernel&amp;m=103269598000900&amp;w=2">Ulrich's benchmark</a> comparing performance of LinuxThreads, NPTL, and IBM's <a href="http://www.kegel.com/c10k.html#threads.ngpt">NGPT</a>.  It seems to show NPTL is much faster than NGPT.
</li></ul>
Here's my try at describing the history of NPTL
(see also <a href="http://www.onlamp.com/pub/a/onlamp/2002/11/07/linux_threads.html">Jerry Cooperstein's article</a>):
<p>
<a href="http://people.redhat.com/drepper/glibcthreads.html">In March 2002,
Bill Abt of the NGPT team, the glibc maintainer Ulrich Drepper, and others met</a> 
to figure out what to do about LinuxThreads.
One idea that came out of the meeting was to improve mutex performance;
Rusty Russell <a href="http://marc.theaimsgroup.com/?l=linux-kernel&amp;m=103284847815916&amp;w=2">et al</a> subsequently implemented
<a href="http://marc.theaimsgroup.com/?l=linux-kernel&amp;m=102196625921110&amp;w=2">fast userspace mutexes (futexes)</a>),
which are now used by both NGPT and NPTL.
Most of the attendees figured NGPT should be merged into glibc.
</p><p>
Ulrich Drepper, though, didn't like NGPT, and figured he could do better.
(For those who have ever tried to contribute a patch to glibc,
this may not come as a big surprise :-)
Over the next few months, Ulrich Drepper, Ingo Molnar,
and others contributed glibc and kernel changes
that make up something called the Native Posix Threads Library (NPTL).
NPTL uses all the kernel enhancements designed for NGPT,
and takes advantage of a few new ones.
Ingo Molnar <a href="https://listman.redhat.com/pipermail/phil-list/2002-September/000013.html">described</a> 
the kernel enhancements as follows:
</p><blockquote><i>
While NPTL uses the three kernel features introduced by NGPT: getpid()  
returns PID, CLONE_THREAD and futexes; NPTL also uses (and relies on) a
much wider set of new kernel features, developed as part of this project.
<p>
Some of the items NGPT introduced into the kernel around 2.5.8 got
modified, cleaned up and extended, such as thread group handling
(CLONE_THREAD). [the CLONE_THREAD changes which impacted NGPT's
compatibility got synced with the NGPT folks, to make sure NGPT does not
break in any unacceptable way.]
</p><p>
The kernel features developed for and used by NPTL are described in the
design whitepaper, http://people.redhat.com/drepper/nptl-design.pdf ...
</p><p>
A short list: TLS support, various clone extensions (CLONE_SETTLS,
CLONE_SETTID, CLONE_CLEARTID), POSIX thread-signal handling, sys_exit()
extension (release TID futex upon VM-release), the sys_exit_group()
system-call, sys_execve() enhancements and support for detached threads.
</p><p>
There was also work put into extending the PID space - eg. procfs crashed
due to 64K PID assumptions, max_pid, and pid allocation scalability work.
Plus a number of performance-only improvements were done as well.
</p></i><p><i>
In essence the new features are a no-compromises approach to 1:1 threading - 
the kernel now helps in everything where it can improve threading, and
we precisely do the minimally necessary set of context switches and kernel
calls for every basic threading primitive.
</i></p></blockquote>
One big difference between the two is that NPTL is a 1:1 threading model,
whereas NGPT is an M:N threading model (see below).  In spite of this,
<a href="https://listman.redhat.com/pipermail/phil-list/2002-September/000009.html">Ulrich's initial benchmarks</a>
seem to show that NPTL is indeed much faster than NGPT.  (The NGPT team
is looking forward to seeing Ulrich's benchmark code to verify the result.)

<h4><a name="threads.freebsd">FreeBSD threading support</a></h4>
FreeBSD supports both LinuxThreads and a userspace threading library.
Also, a M:N implementation called KSE was introduced in FreeBSD 5.0.
For one overview, see <a href="http://www.unobvious.com/bsd/freebsd-threads.html">www.unobvious.com/bsd/freebsd-threads.html</a>.
<p>
On 25 Mar 2003, 
<a href="http://docs.freebsd.org/cgi/getmsg.cgi?fetch=121207+0+archive/2003/freebsd-arch/20030330.freebsd-arch">Jeff Roberson posted on freebsd-arch</a>:
</p><blockquote><i>
... Thanks to the foundation provided by Julian, David Xu, Mini, Dan Eischen,
and everyone else who has participated with KSE and libpthread development
Mini and I have developed a 1:1 threading implementation.  This code works
in parallel with KSE and does not break it in any way.  It actually helps
bring M:N threading closer by testing out shared bits. ...
</i></blockquote>
And in July 2006, 
<a href="http://marc.theaimsgroup.com/?l=freebsd-threads&amp;m=115191979412894&amp;w=2">Robert Watson proposed that the 1:1 threading
implementation become the default in FreeBsd 7.x</a>:
<blockquote><i>
I know this has been discussed in the past, but I figured with 7.x trundling 
forward, it was time to think about it again.  In benchmarks for many common 
applications and scenarios, libthr demonstrates significantly better 
performance over libpthread...
libthr is also implemented across a larger number of our 
platforms, and is already libpthread on several.  The first recommendation we 
make to MySQL and other heavy thread users is "Switch to libthr", which is 
suggestive, also!  ...
So the strawman proposal is: make libthr the default threading library on 7.x. 
</i></blockquote>

<h4><a name="threads.netbsd">NetBSD threading support</a></h4>
According to a note from Noriyuki Soda:
<blockquote><i>
Kernel supported M:N thread library based on the Scheduler
Activations model is merged into NetBSD-current on Jan 18 2003.
</i></blockquote>
For details, see 
<a href="http://web.mit.edu/nathanw/www/usenix/freenix-sa/">An Implementation of Scheduler Activations on the NetBSD Operating System</a> by Nathan J. Williams, Wasabi Systems, Inc., presented at FREENIX '02.

<h4><a name="threads.solaris">Solaris threading support</a></h4>
The thread support in Solaris is evolving... from Solaris 2 to Solaris 8, the default
threading library used an M:N model, but Solaris 9 defaults to 1:1 model thread support.
See <a href="http://docs.sun.com/db/doc/805-5080">Sun's multithreaded programming guide</a>
and <a href="http://java.sun.com/docs/hotspot/threads/threads.html">Sun's note about Java and Solaris threading</a>.

<h4><a name="threads.java">Java threading support in JDK 1.3.x and earlier</a></h4>
As is well known, Java up to JDK1.3.x did not support any method of 
handling network connections other than one thread per client.
<a href="http://www.volano.com/report/">Volanomark</a> is a good microbenchmark 
which measures throughput in messsages per second at various
numbers of simultaneous connections.  As of May 2003, JDK 1.3
implementations from various vendors are in fact able to handle
ten thousand simultaneous connections -- albeit with significant
performance degradation.  See <a href="http://www.volano.com/report/#nettable">Table 4</a>
for an idea of which JVMs can handle 10000 connections, and how
performance suffers as the number of connections increases.

<h4><a name="1:1">Note: 1:1 threading vs. M:N threading</a></h4>
There is a choice when implementing a threading library: you can either
put all the threading support in the kernel (this is called the 1:1 threading
model), or you can move a fair bit of it into userspace (this is called the M:N
threading model).  At one point, M:N was thought to be higher performance,
but it's so complex that it's hard to get right, and most people are moving away from it.
<ul>
<li><a href="http://marc.theaimsgroup.com/?l=linux-kernel&amp;m=103284879216107&amp;w=2">Why Ingo Molnar prefers 1:1 over M:N</a>
</li><li><a href="http://java.sun.com/docs/hotspot/threads/threads.html">Sun
is moving to 1:1 threads</a>
</li><li><a href="http://www-124.ibm.com/pthreads/">NGPT</a> is an M:N threading library for Linux.
</li><li>Although <a href="http://people.redhat.com/drepper/glibcthreads.html">Ulrich
Drepper planned to use M:N threads in the new glibc threading library</a>,
he has since <a href="http://people.redhat.com/drepper/nptl-design.pdf">switched to the 1:1 threading model.</a>
</li><li><a href="http://developer.apple.com/technotes/tn/tn2028.html#MacOSXThreading">MacOSX
appears to use 1:1 threading.</a>  
</li><li><a href="http://people.freebsd.org/~julian/">FreeBSD</a> and 
<a href="http://web.mit.edu/nathanw/www/usenix/freenix-sa/">NetBSD</a> 
appear to still believe in M:N threading...  The lone holdouts?
Looks like freebsd 7.0 might switch to 1:1 threading (see above), so 
perhaps M:N threading's believers have finally been proven wrong
everywhere.
</li></ul>

<h3><a name="kio">5. Build the server code into the kernel</a></h3>
<p>
Novell and Microsoft are both said to have done this at various times,
at least one NFS implementation does this, 
<a href="http://www.fenrus.demon.nl/">khttpd</a> does this for Linux 
and static web pages, and 
<a href="http://slashdot.org/comments.pl?sid=00/07/05/0211257&amp;cid=218">"TUX" (Threaded linUX webserver)</a>
is a blindingly fast and flexible kernel-space HTTP server by Ingo Molnar for Linux.
Ingo's <a href="http://marc.theaimsgroup.com/?l=linux-kernel&amp;m=98098648011183&amp;w=2">September 1, 2000 announcement</a>
says an alpha version of TUX can be downloaded from 
<a href="ftp://ftp.redhat.com/pub/redhat/tux">ftp://ftp.redhat.com/pub/redhat/tux</a>,
and explains how to join a mailing list for more info.

<br>
The linux-kernel list has been discussing the pros and cons of this
approach, and the consensus seems to be instead of moving web servers 
into the kernel, the kernel should have the smallest possible hooks added
to improve web server performance.  That way, other kinds of servers
can benefit.  See e.g. 
<a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9906_03/msg01041.html">Zach Brown's remarks</a>
about userland vs. kernel http servers.  
It appears that the 2.4 linux kernel provides sufficient power to user programs, as
the <a href="http://www.kegel.com/c10k.html#x15">X15</a> server runs about as fast as Tux, but doesn't use any
kernel modifications.

</p><h2><a name="userspace">Bring the TCP stack into userspace</a></h2>
See for instance the
<a href="http://info.iet.unipi.it/~luigi/netmap/">netmap</a> packet I/O
framework, and the
<a href="http://conferences.sigcomm.org/hotnets/2013/papers/hotnets-final43.pdf">Sandstorm</a>
proof-of-concept web server based on it.
<p>
</p><h2><a name="comments">Comments</a></h2>
<p>
Richard Gooch has written 
<a href="http://www.atnf.csiro.au/~rgooch/linux/docs/io-events.html">a paper discussing I/O options</a>.
</p><p>
In 2001, Tim Brecht and MMichal Ostrowski 
<a href="http://www.hpl.hp.com/techreports/2001/HPL-2001-314.html">measured various strategies</a> 
for simple select-based servers.  Their data is worth a look.
</p><p>
In 2003, Tim Brecht posted 
<a href="http://www.hpl.hp.com/research/linux/userver/">source code for userver</a>,
a small web server put together from several servers written by
Abhishek Chandra, David Mosberger, David Pariag, and Michal Ostrowski. 
It can use select(), poll(), epoll(), or sigio.  
</p><p>
Back in March 1999, 
<a href="http://marc.theaimsgroup.com/?l=apache-httpd-dev&amp;m=92100977123493&amp;w=2">Dean Gaudet posted</a>:
</p><blockquote><i>
I keep getting asked "why don't you guys use a select/event based model
like Zeus?  It's clearly the fastest." 
...
</i></blockquote>
His reasons boiled down to "it's really hard, and the payoff isn't clear".
Within a few months, though, it became clear that people were willing to work on it.
<p>
Mark Russinovich wrote 
<a href="http://linuxtoday.com/stories/5499.html">an editorial</a> and
<a href="http://www.winntmag.com/Articles/Index.cfm?ArticleID=5048">an article</a>
discussing I/O strategy issues in the 2.2 Linux kernel.  Worth reading, even
he seems misinformed on some points.  In particular, he
seems to think that Linux 2.2's asynchronous I/O 
(see F_SETSIG above) doesn't notify the user process when data is ready, only
when new connections arrive.  This seems like a bizarre misunderstanding.
See also 
<a href="http://www.dejanews.com/getdoc.xp?AN=431444525">comments on an earlier draft</a>,
<a href="http://www.dejanews.com/getdoc.xp?AN=472893693">Ingo Molnar's rebuttal of 30 April 1999</a>,
<a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9905_01/msg00089.html">Russinovich's comments of 2 May 1999</a>,
<a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9905_01/msg00263.html">a rebuttal</a> from Alan Cox,
and various 
<a href="http://www.dejanews.com/dnquery.xp?ST=PS&amp;QRY=threads&amp;DBS=1&amp;format=threaded&amp;showsort=score&amp;maxhits=100&amp;LNG=ALL&amp;groups=fa.linux.kernel+&amp;fromdate=jun+1+1998">posts to linux-kernel</a>.  
I suspect he was trying to say that Linux doesn't support asynchronous disk I/O,
which used to be true, but now that SGI has implemented <a href="http://www.kegel.com/c10k.html#aio">KAIO</a>,
it's not so true anymore.
</p><p>
See these pages at <a href="http://www.sysinternals.com/ntw2k/info/comport.shtml">sysinternals.com</a> and 
<a href="http://msdn.microsoft.com/library/techart/msdn_scalabil.htm">MSDN</a> for information
on "completion ports", which he said were unique to NT; in a nutshell,
win32's "overlapped I/O" turned out to be too low level to be convenient, and
a "completion port" is a wrapper that provides a queue of completion events,
plus scheduling magic that tries to keep the number of running threads constant
by allowing more threads to pick up completion events if other threads that
had picked up completion events from this port are sleeping (perhaps doing blocking I/O).
</p><p>
See also <a href="http://www.as400.ibm.com/developer/v4r5/api.html">OS/400's support for I/O completion ports</a>.
</p><p>
<a name="15k">There</a> was an interesting discussion on linux-kernel in September 1999 titled
"<a href="http://www.cs.helsinki.fi/linux/linux-kernel/Year-1999/1999-36/0160.html">&gt; 15,000 Simultaneous Connections</a>"
(and the <a href="http://www.cs.helsinki.fi/linux/linux-kernel/Year-1999/1999-37/0612.html">second week</a> of the thread).
Highlights:
</p><ul>
<li>
Ed Hall 
<a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_01/msg00807.html">posted</a> a few notes on his experiences; he's achieved 
&gt;1000 connects/second on a UP P2/333 running Solaris.  His code
used a small pool of threads (1 or 2 per CPU) each managing a large number 
of clients using "an event-based model".
</li><li>Mike Jagdis <a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_01/msg00831.html">posted an analysis of poll/select overhead</a>, and said
"The current select/poll implementation can be improved significantly,
especially in the blocking case, but the overhead will still increase
with the number of descriptors because select/poll does not, and
cannot, remember what descriptors are interesting. This would be
easy to fix with a new API.  Suggestions are welcome..."
</li><li>Mike <a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_01/msg00964.html">posted</a> about his <a href="http://www.purplet.demon.co.uk/linux/select/">work on improving select() and poll()</a>.
</li><li>Mike <a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_01/msg00971.html">posted a bit about a possible API to replace poll()/select()</a>: 
"How about a 'device like' API where you write 'pollfd like' structs,
the 'device' listens for events and delivers 'pollfd like' structs
representing them when you read it? ... "
</li><li>Rogier Wolff 
<a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_01/msg00979.html">suggested</a>
using "the API that the digital guys suggested", 
<a href="http://www.cs.rice.edu/~gaurav/papers/usenix99.ps">http://www.cs.rice.edu/~gaurav/papers/usenix99.ps</a>
</li><li>Joerg Pommnitz <a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_02/msg00001.html">pointed out</a> that any new API along these lines should
be able to wait for not just file descriptor events, but also signals and maybe 
SYSV-IPC.  Our synchronization primitives should certainly be able to
do what Win32's WaitForMultipleObjects can, at least.
</li><li>Stephen Tweedie <a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_02/msg01198.html">asserted</a> that the combination of F_SETSIG, queued realtime
signals, and sigwaitinfo() was a superset of the API proposed in 
http://www.cs.rice.edu/~gaurav/papers/usenix99.ps.  He also mentions that
you keep the signal blocked at all times if you're interested in performance;
instead of the signal being delivered asynchronously, the process grabs the
next one from the queue with sigwaitinfo().
</li><li>Jayson Nordwick <a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_03/msg00002.html">compared</a>
completion ports with the F_SETSIG synchronous event model,
and concluded they're pretty similar.
</li><li>Alan Cox <a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_03/msg00043.html">noted</a> that an older rev of SCT's SIGIO patch is included in
2.3.18ac.
</li><li>Jordan Mendelson <a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_03/msg00093.html">posted</a> some example code showing how to use F_SETSIG.
</li><li>Stephen C. Tweedie <a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_03/msg00095.html">continued</a> the comparison of completion ports and F_SETSIG,
and noted: "With a signal dequeuing mechanism, your application is going to get
signals destined for various library components if libraries are using
the same mechanism," but the library can set up its own signal handler,
so this shouldn't affect the program (much).
</li><li><a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_04/msg00900.html">Doug Royer</a> 
noted that he'd gotten 100,000 connections on Solaris 2.6 while he
was working on the Sun calendar server.
Others chimed in with estimates of how much RAM that would require
on Linux, and what bottlenecks would be hit.
</li></ul>
<p>
Interesting reading!
</p><p>

</p><h2><a name="limits.filehandles">Limits on open filehandles</a></h2>
<ul>
<li>Any Unix: the limits set by ulimit or setrlimit.
</li><li>Solaris: see <a href="http://www.wins.uva.nl/pub/solaris/solaris2/Q3.46.html">the Solaris FAQ,
question 3.46</a> (or thereabouts; they renumber the questions periodically).
</li><li>FreeBSD:<br>
<br>Edit /boot/loader.conf, add the line
<pre>set kern.maxfiles=XXXX</pre>
where XXXX is the desired system limit on file descriptors,
and reboot.  Thanks to an anonymous reader, who wrote in to
say he'd achieved far more than 10000 connections on FreeBSD 4.3,
and says 
<blockquote>
"FWIW: You can't actually tune the maximum number of connections
in FreeBSD trivially, via sysctl....  You have to do it in the 
/boot/loader.conf file.
<br>
The reason for this is that the zalloci() calls for initializing
the sockets and tcpcb structures zones occurs very early in
system startup, in order that the zone be both type stable and
that it be swappable.
<br>
You will also need to set the number of mbufs much higher, since
you will (on an unmodified kernel) chew up one mbuf per connection
for tcptempl structures, which are used to implement keepalive."
</blockquote>
Another reader says
<blockquote>
"As of FreeBSD 4.4, the tcptempl structure is no
longer allocated; you no longer have to worry about one mbuf being chewed
up per connection."
</blockquote>
See also:
<ul>
<li>
<a href="http://www.freebsd.org/doc/en_US.ISO8859-1/books/handbook/configtuning-kernel-limits.html">the FreeBSD handbook</a>
</li><li>
<a href="http://www.freebsd.org/cgi/man.cgi?query=tuning#SYSCTL+TUNING">SYSCTL TUNING</a>,
<a href="http://www.freebsd.org/cgi/man.cgi?query=tuning#LOADER+TUNABLES">LOADER TUNABLES</a>, and
<a href="http://www.freebsd.org/cgi/man.cgi?query=tuning#KERNEL+CONFIG+TUNING">KERNEL CONFIG TUNING</a> in 'man tuning'
</li><li><a href="http://www.daemonnews.org/200108/benchmark.html">The Effects of Tuning a FreeBSD 4.3 Box for High Performance</a>, Daemon News, Aug 2001
</li><li><a href="http://www.postfix.org/faq.html#moby-freebsd">postfix.org tuning notes</a>, covering FreeBSD 4.2 and 4.4
</li><li><a href="http://www.measurement-factory.com/docs/FreeBSD/">the Measurement Factory's notes</a>, circa FreeBSD 4.3
</li></ul>

</li><li>OpenBSD:
A reader says
<blockquote>
"In OpenBSD, an additional tweak is required to increase the number of
open filehandles available per process: the openfiles-cur parameter in
<a href="http://www.freebsd.org/cgi/man.cgi?query=login.conf&amp;manpath=OpenBSD+3.1">/etc/login.conf</a>
needs to be increased. You can change kern.maxfiles
either with sysctl -w or in sysctl.conf but it has no effect. This
matters because as shipped, the login.conf limits are a quite low 64
for nonprivileged processes, 128 for privileged."
</blockquote>

</li><li>Linux: See <a href="http://asc.di.fct.unl.pt/~jml/mirror/Proc/">Bodo Bauer's /proc documentation</a>.
On 2.4 kernels:
<pre>echo 32768 &gt; /proc/sys/fs/file-max
</pre>
increases the system limit on open files, and
<pre>ulimit -n 32768</pre>
increases the current process' limit.   
<p>
On 2.2.x kernels,
</p><pre>echo 32768 &gt; /proc/sys/fs/file-max
echo 65536 &gt; /proc/sys/fs/inode-max
</pre>
increases the system limit on open files, and
<pre>ulimit -n 32768</pre>
increases the current process' limit.   
<p>
I verified that a process on Red Hat 6.0 
(2.2.5 or so plus patches) can open at least 31000 file descriptors this way.
Another fellow has verified that a process on 2.2.12 can open at least 
90000 file descriptors this way (with appropriate limits).   The upper bound
seems to be available memory.
<br>
Stephen C. Tweedie <a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_02/msg01092.html">posted</a>
about how to set ulimit limits globally or per-user at boot time using
initscript and pam_limit.
<br>
In older 2.2 kernels, though, the number of open files per process is 
still limited to 1024, even with the above changes.
<br>
See also 
<a href="http://www.dejanews.com/getdoc.xp?AN=313316592">Oskar's 1998 post</a>,
which talks about the per-process and system-wide limits on file descriptors
in the 2.0.36 kernel.
</p></li></ul>

<h2><a name="limits.threads">Limits on threads</a></h2>
<p>
On any architecture, you may need to reduce the amount
of stack space allocated for each thread to avoid running
out of virtual memory.  You can set this at runtime with
pthread_attr_init() if you're using pthreads.
</p><ul>
<li>Solaris: it supports as many threads as will fit in memory, I hear.
</li><li>Linux 2.6 kernels with NPTL: /proc/sys/vm/max_map_count may need to be increased to go above 32000 or so threads.
(You'll need to use very small stack threads to get anywhere near that number of threads, though,
unless you're on a 64 bit processor.)  See the NPTL mailing list, e.g. the thread 
with subject "<a href="https://listman.redhat.com/archives/phil-list/2003-August/msg00005.html">Cannot create more than 32K threads?</a>",
for more info.
</li><li>Linux 2.4: /proc/sys/kernel/threads-max is the max number of threads;
it defaults to 2047 on my Red Hat 8 system.
You can set increase this as usual by echoing new values into that file,
e.g. "echo 4000 &gt;  /proc/sys/kernel/threads-max"
</li><li>Linux 2.2: Even the 2.2.13 kernel limits the number of threads,
at least on Intel.  I don't know what the limits are on other architectures.
<a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9812_02/msg00048.html">Mingo posted a patch
for 2.1.131 on Intel</a> that removed this limit.  It appears to be integrated into 2.3.20.
<p>
See also <a href="http://www.volano.com/linux.html">Volano's detailed
instructions for raising file, thread, and FD_SET limits in the 2.2 kernel</a>.  Wow.
This document steps you through a lot of stuff that
would be hard to figure out yourself, but is somewhat dated.
</p></li><li>Java: See <a href="http://www.volano.com/benchmarks.html">Volano's detailed benchmark info</a>,
plus their <a href="http://www.volano.com/server.html">info on how to tune various systems</a>
to handle lots of threads.
</li></ul>

<h2><a name="java">Java issues</a></h2>
<p>
Up through JDK 1.3, Java's standard networking libraries mostly offered the 
<a href="http://www.kegel.com/c10k.html#threaded.java">one-thread-per-client model</a>.
There was a way to do nonblocking reads, but no way to do nonblocking writes.
</p><p>
In May 2001, <a href="http://java.sun.com/j2se/1.4/">JDK 1.4</a> introduced
the package <a href="http://java.sun.com/j2se/1.4/docs/guide/nio/">java.nio</a>
to provide full support for nonblocking I/O (and some other goodies).
See <a href="http://java.sun.com/j2se/1.4/relnotes.html#nio">the release notes</a> for some caveats.
Try it out and give Sun feedback!
</p><p>
HP's java also includes a <a href="http://www.devresource.hp.com/JavaATC/JavaPerfTune/pollapi.html">Thread Polling API</a>.
</p><p>
In 2000, Matt Welsh implemented nonblocking sockets for Java; his performance
benchmarks show that they have advantages over blocking sockets in servers
handling many (up to 10000) connections.  His class library is called
<a href="http://www.cs.berkeley.edu/~mdw/proj/java-nbio/">java-nbio</a>;
it's part of the 
<a href="http://www.cs.berkeley.edu/~mdw/proj/sandstorm/">Sandstorm</a> project.
Benchmarks showing 
<a href="http://www.cs.berkeley.edu/~mdw/proj/sandstorm/iocore-bench/">performance with 10000 connections</a> are available.
</p><p>
See also 
<a href="http://arctic.org/~dean/hybrid-jvm.txt">Dean Gaudet's essay</a>
on the subject of Java, network I/O, and threads, and the 
<a href="http://www.cs.berkeley.edu/~mdw/papers/events.pdf">paper</a> by Matt Welsh
on events vs. worker threads.  
</p><p>
Before NIO, there were several proposals for improving Java's networking APIs:
</p><ul>
<li>Matt Welsh's 
<a href="http://www.cs.berkeley.edu/~mdw/proj/jaguar/">Jaguar system</a>
proposes preserialized objects, new Java bytecodes, and memory management
changes to allow the use of asynchronous I/O with Java.  
</li><li><a href="http://www.cs.cornell.edu/Info/People/chichao/papers.htm">Interfacing
Java to the Virtual Interface Architecture</a>, by C-C. Chang and
T. von Eicken, proposes memory management changes to allow the use
of asynchronous I/O with Java.
</li><li>
<a href="http://java.sun.com/aboutJava/communityprocess/jsr/jsr_051_ioapis.html">JSR-51</a>
was the Sun project that came up with the java.nio package.  
Matt Welsh participated (who says Sun doesn't listen?).
</li></ul>

<h2><a name="tips">Other tips</a></h2>
<ul>
<li><a name="zerocopy">Zero-Copy</a><br>
Normally, data gets copied many times on its way from here to there.
Any scheme that eliminates these copies to the bare physical minimum is called "zero-copy".
<ul> 
<li><a href="http://marc.theaimsgroup.com/?l=linux-kernel&amp;m=104121076420067&amp;w=2">Thomas Ogrisegg's zero-copy send patch</a> for mmaped files under Linux 2.4.17-2.4.20.  Claims it's faster than sendfile().
</li><li><a href="http://www.usenix.org/publications/library/proceedings/osdi99/full_papers/pai/pai_html/pai.html">IO-Lite</a> 
is a proposal for a set of I/O primitives that gets rid of the need for many copies.
</li><li>
<a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9905_01/msg00263.html">Alan Cox noted that
zero-copy is sometimes not worth the trouble</a> back in 1999.  (He did like sendfile(), though.)
</li><li>Ingo <a href="http://boudicca.tux.org/hypermail/linux-kernel/2000week36/0979.html">implemented
a form of zero-copy TCP</a> in the 2.4 kernel for TUX 1.0 in July 2000, and says he'll make it available to userspace soon.
</li><li><a href="http://people.freebsd.org/~ken/zero_copy/">Drew Gallatin and Robert Picco have added 
some zero-copy features to FreeBSD</a>; the idea seems to be that
if you call write() or read() on a socket, the pointer is page-aligned,
and the amount of data transferred is at least a page, *and* you
don't immediately reuse the buffer, memory management tricks will be
used to avoid copies.  But see 
<a href="http://boudicca.tux.org/hypermail/linux-kernel/2000week39/0249.html">followups to this message on linux-kernel</a>
for people's misgivings about the speed of those memory management tricks.
<p>
According to a note from Noriyuki Soda:
</p><blockquote><i>
Sending side zero-copy is supported since NetBSD-1.6 release by
specifying "SOSEND_LOAN" kernel option. This option is now default
on NetBSD-current (you can disable this feature by specifying
"SOSEND_NO_LOAN" in the kernel option on NetBSD_current).
With this feature, zero-copy is automatically enabled, if data more
than 4096 bytes are specified as data to be sent.
</i></blockquote>

</li><li><a name="sendfile">The sendfile() system call can implement zero-copy networking.</a><br>
The sendfile() function in Linux and FreeBSD lets you tell the kernel to send part 
or all of a file.  This lets the OS do it as efficiently as possible.
It can be used equally well in servers using threads or servers using
nonblocking I/O.  (In Linux, it's poorly documented at the moment; <a href="http://www.dejanews.com/getdoc.xp?AN=422899634">use _syscall4 to 
call it</a>.  Andi Kleen is writing new man pages that cover this.
See also <a href="http://www.linuxgazette.com/issue91/tranter.html">Exploring The sendfile System Call</a> by Jeff Tranter in Linux Gazette issue 91.)  

<a href="http://www.dejanews.com/getdoc.xp?AN=423005088">Rumor has it</a>, 
ftp.cdrom.com benefitted noticeably from sendfile().
<p>A zero-copy implementation of sendfile() is on its way for the 2.4 kernel.
See <a href="http://lwn.net/2001/0125/kernel.php3">LWN Jan 25 2001</a>.
</p><p>One developer using sendfile() with Freebsd reports that using
POLLWRBAND instead of POLLOUT makes a big difference.  
</p><p>
Solaris 8 (as of the July 2001 update) has a new system call 'sendfilev'.
<a href="http://www.kegel.com/sendfilev.txt">A copy of the man page is here.</a>.
The Solaris 8 7/01 
<a href="http://www.sun.com/software/solaris/fcc/ucc-details701.html">release notes</a>
also mention it.  I suspect that this will be most useful when sending
to a socket in blocking mode; it'd be a bit of a pain to use with a nonblocking
socket.
</p></li></ul>

</li><li><a name="writev">Avoid small frames by using writev (or TCP_CORK)</a><br>
A new socket option under Linux, TCP_CORK, tells the kernel to
avoid sending partial frames, which helps a bit e.g. when there are
lots of little write() calls you can't bundle together for some reason.
Unsetting the option flushes the buffer.  Better to use writev(), though...
<p>
See <a href="http://lwn.net/2001/0125/kernel.php3">LWN Jan 25 2001</a> for a summary
of some very interesting discussions on linux-kernel about TCP_CORK and 
a possible alternative MSG_MORE.
</p></li><li><a name="overload">Behave sensibly on overload.</a><br>
[<a href="http://www.citi.umich.edu/techreports/reports/citi-tr-00-7.ps.gz">Provos, Lever, and Tweedie 2000</a>] 
notes that dropping incoming connections when the server
is overloaded improved the shape of the performance curve,
and reduced the overall error rate.  They used a smoothed
version of "number of clients with I/O ready" as a measure
of overload.  This technique should be easily applicable to
servers written with select, poll, or any system call that returns
a count of readiness events per call (e.g. /dev/poll or sigtimedwait4()).
</li><li><a name="nativethreads">Some programs can benefit from using non-Posix threads.</a><br>
Not all threads are created equal.  The clone() function in Linux
(and its friends in other operating systems)
lets you create a thread that has its own current working directory,
for instance, which can be very helpful when implementing an ftp server.
See Hoser FTPd for an example of the use of native threads rather than pthreads.
</li><li><a name="caching">Caching your own data can sometimes be a win.</a><br>
"Re: fix for hybrid server problems" by Vivek Sadananda Pai 
(vivek@cs.rice.edu) on 
<a href="http://www.humanfactor.com/cgi-bin/cgi-delegate/apache-ML/nh/1999/">new-httpd</a>, May 9th, states:
<blockquote> 
<p>
"I've compared the raw performance of a select-based server with a
multiple-process server on both FreeBSD and Solaris/x86. On
microbenchmarks, there's only a marginal difference in performance
stemming from the software architecture. The big performance win for
select-based servers stems from doing application-level caching. While
multiple-process servers can do it at a higher cost, it's harder to
get the same benefits on real workloads (vs microbenchmarks).
I'll be presenting those measurements as part of a paper that'll
appear at the next Usenix conference. If you've got postscript,
the paper is available at 
<a href="http://www.cs.rice.edu/~vivek/flash99/">http://www.cs.rice.edu/~vivek/flash99/</a>"
</p></blockquote>
</li></ul>

<h2><a name="limits.other">Other limits</a></h2>
<ul>
<li>Old system libraries might use 16 bit variables to hold
file handles, which causes trouble above 32767 handles.  glibc2.1 should be ok.
</li><li>Many systems use 16 bit variables to hold process or thread id's.
It would be interesting to port the <a href="http://www.volano.com/benchmarks.html">Volano scalability
benchmark</a> to C, and see what the upper limit on number of threads is for the various operating systems.
</li><li>Too much thread-local memory is preallocated by some operating systems;
if each thread gets 1MB, and total VM space is 2GB, that creates an upper limit
of 2000 threads.
</li><li>Look at the performance comparison graph at the bottom of
<a href="http://www.acme.com/software/thttpd/benchmarks.html">http://www.acme.com/software/thttpd/benchmarks.html</a>.
Notice how various servers have trouble above 128 connections, even on Solaris 2.6?  
Anyone who figures out why, let me know.  
<br>
Note: if the TCP stack has a bug that causes a short (200ms)
delay at SYN or FIN time, as Linux 2.2.0-2.2.6 had, and the OS or
http daemon has a hard limit on the number of connections open,
you would expect exactly this behavior.  There may be other causes.
</li></ul>

<h2><a name="kernel">Kernel Issues</a></h2> 
<p>
For Linux, it looks like kernel bottlenecks are being fixed constantly.
See <a href="http://lwn.net/">Linux Weekly News</a>,
<a href="http://www.kt.opensrc.org/">Kernel Traffic</a>,
<a href="http://marc.theaimsgroup.com/?l=linux-kernel">the Linux-Kernel mailing list</a>,
and <a href="http://www.kegel.com/mindcraft_redux.html">my Mindcraft Redux page</a>.
</p><p>
In March 1999, Microsoft sponsored a benchmark comparing NT to Linux
at serving large numbers of http and smb clients, in which they
failed to see good results from Linux.  
See also <a href="http://www.kegel.com/mindcraft_redux.html">my article on Mindcraft's April 1999 Benchmarks</a>
for more info.
</p><p>
See also <a href="http://www.citi.umich.edu/projects/citi-netscape/">The Linux Scalability Project</a>.
They're doing interesting work, including <a href="http://linuxwww.db.erau.edu/mail_archives/linux-kernel/May_99/4105.html">
Niels Provos' hinting poll patch</a>, and some work on
the <a href="http://www.citi.umich.edu/projects/linux-scalability/reports/accept.html">thundering herd problem</a>.
</p><p>
See also <a href="http://www.purplet.demon.co.uk/linux/select/">Mike Jagdis' work on improving select() and poll()</a>; here's <a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9909_01/msg00964.html">Mike's post</a> about it.
</p><p>
<a href="http://www.linuxhq.com/lnxlists/linux-kernel/lk_9910_02/msg00889.html">Mohit Aron (aron@cs.rice.edu) 
writes that rate-based clocking in TCP can improve HTTP response time over 'slow' connections by 80%.</a>

</p><h2><a name="benchmarking">Measuring Server Performance</a></h2>
<p>
Two tests in particular are simple, interesting, and hard: 
</p><ol>
<li>raw connections per second (how many 512 byte files per second can you
serve?)
</li><li>total transfer rate on large files with many slow clients
(how many 28.8k modem clients can simultaneously download
from your server before performance goes to pot?)
</li></ol>
<p>
Jef Poskanzer has published benchmarks comparing many web servers.
See <a href="http://www.acme.com/software/thttpd/benchmarks.html">http://www.acme.com/software/thttpd/benchmarks.html</a>  
for his results.
</p><p>
I also have 
<a href="http://www.alumni.caltech.edu/~dank/fixing-overloaded-web-server.html">
a few old notes about comparing thttpd to Apache</a> that may be of interest
to beginners.
</p><p>
<a href="http://linuxhq.com/lnxlists/linux-kernel/lk_9906_02/msg00248.html">Chuck Lever keeps reminding us</a> about 
<a href="http://www.cs.rice.edu/CS/Systems/Web-measurement/paper/paper.html">Banga and Druschel's paper on web server benchmarking</a>.  It's worth a read.
</p><p>
IBM has an excellent paper titled <a href="http://www.research.ibm.com/journal/sj/391/baylor.html">Java server benchmarks</a> [Baylor et al, 2000].  It's worth a read.

</p><h1><a name="examples">Examples</a></h1>

<a href="http://nginx.org/">Nginx</a> is a web server that uses whatever
high-efficiency network event mechanism is available on the target OS.
It's getting popular; there are even  <a href="http://www.amazon.com/Nginx-HTTP-Server-Cl%C3%A9ment-Nedelcu/dp/1849510865/">books</a> about it
(and since this page was originally written, many more, including a <a href="https://www.amazon.com/Nginx-HTTP-Server-Harness-infrastructure/dp/178862355X">fourth edition</a> of that book.)

<h2><a name="examples.nb.select">Interesting select()-based servers</a></h2>
<ul>
<li><a href="http://www.acme.com/software/thttpd/">thttpd</a>
Very simple.  Uses a single process.  It has good performance,
but doesn't scale with the number of CPU's.  Can also use kqueue.
</li><li><a href="http://mathop.diva.nl/">mathopd</a>.  Similar to thttpd.
</li><li><a href="http://www.fhttpd.org/">fhttpd</a>
</li><li><a href="http://www.boa.org/">boa</a>
</li><li><a href="http://www.roxen.com/">Roxen</a>
</li><li><a href="http://www.zeustech.net/">Zeus</a>, a commercial server that tries to be the absolute fastest.
See their <a href="http://support.zeustech.net/faq/entries/tuning.html">tuning guide</a>.   
</li><li>The other non-Java servers listed at <a href="http://www.acme.com/software/thttpd/benchmarks.html">http://www.acme.com/software/thttpd/benchmarks.html</a>
</li><li><a href="http://ca.us.mirrors.freshmeat.net/appindex/1999/02/17/919251275.html">BetaFTPd</a>
</li><li><a href="http://www.cs.rice.edu/~vivek/iol98/">Flash-Lite</a> -
web server using IO-Lite.
</li><li><a href="http://www.cs.rice.edu/~vivek/flash99/">Flash: An efficient and portable Web server</a> -- uses select(), mmap(), mincore()
</li><li><a href="http://www.cs.princeton.edu/~yruan/debox/">The Flash web server as of 2003</a> -- uses select(), modified sendfile(), async open()
</li><li><a href="http://www.imatix.com/html/xitami/">xitami</a> - uses select() to
implement its own thread abstraction for portability to systems without 
threads.
</li><li><a href="http://www.nightmare.com/medusa/medusa.html">Medusa</a> - a server-writing toolkit in Python that tries to deliver very high performance. 
</li><li><a href="http://www.hpl.hp.com/research/linux/userver/">userver</a> - a small http server that can use select, poll, epoll, or sigio
</li></ul>


<h2><a name="examples.nb./dev/poll">Interesting /dev/poll-based servers</a></h2>
<ul>
<li>
<i>N. Provos, C. Lever</i>,
<a href="http://www.citi.umich.edu/techreports/reports/citi-tr-00-4.pdf">"Scalable Network I/O in Linux,"</a>
May, 2000. [FREENIX track, Proc. USENIX 2000, San Diego, California (June, 2000).]  Describes a
version of thttpd modified to support /dev/poll.  Performance is compared
with phhttpd.
</li></ul>

<h2><a name="examples.nb.epoll">Interesting epoll-based servers</a></h2>
<ul>
<li><a href="https://github.com/Adaptv/ribs2">ribs2</a>
</li><li><a href="http://bogomips.org/cmogstored/README">cmogstored</a> - uses
epoll/kqueue for most networking, threads for disk and accept4
</li></ul>

<h2><a name="examples.nb.kqueue">Interesting kqueue()-based servers</a></h2>
<ul>
<li><a href="http://www.acme.com/software/thttpd/">thttpd</a> (as of version 2.21?)
</li><li>
Adrian Chadd says "I'm doing a lot of work to make squid actually LIKE a kqueue IO system";
it's an official Squid subproject; see 
<a href="http://squid.sourceforge.net/projects.html#commloops">http://squid.sourceforge.net/projects.html#commloops</a>.
(This is apparently newer than <a href="http://www.advogato.org/person/benno/">Benno</a>'s 
<a href="http://netizen.com.au/~benno/squid-select.tar.gz">patch</a>.)

</li></ul>

<h2><a name="examples.nb.sigio">Interesting realtime signal-based servers</a></h2>
<ul>
<li><a name="x15">Chromium's</a> X15.
This uses the 2.4 kernel's SIGIO feature together with sendfile() and TCP_CORK, and 
reportedly achieves higher speed than even TUX.
The <a href="http://www.chromium.com/cgi-bin/crosforum/YaBB.pl">source is available</a> under
a community source (not open source) license.  See 
<a href="http://boudicca.tux.org/hypermail/linux-kernel/2001week21/1624.html">the original announcement</a>
by Fabio Riccardi.

</li><li><a name="phhttpd">Zach Brown's</a> <a href="http://www.zabbo.net/phhttpd/">phhttpd</a> - "a
quick web server that was written to showcase the sigio/siginfo event
model. consider this code highly
experimental and yourself highly mental if you try and use it in a production environment."
Uses the <a href="http://www.kegel.com/c10k.html#nb.sigio">siginfo</a> features of 2.3.21 or later, and includes the needed patches
for earlier kernels.
Rumored to be even faster than khttpd.
See <a href="http://www.cs.helsinki.fi/linux/linux-kernel/Year-1999/1999-22/0453.html">his post of 31 May 1999</a>
for some notes.
</li></ul>

<h2><a name="examples.threaded">Interesting thread-based servers</a></h2>
<ul>
<li><a href="http://www.zabbo.net/hftpd/">Hoser FTPD</a>.
See their <a href="http://www.zabbo.net/hftpd/bench.html">benchmark page</a>.
</li><li><a href="http://ca.us.mirrors.freshmeat.net/appindex/1999/02/06/918317238.html">Peter Eriksson's phttpd</a> and </li><li><a href="http://ca.us.mirrors.freshmeat.net/appindex/1999/02/06/918313631.html">pftpd</a> 
</li><li>The Java-based servers listed at <a href="http://www.acme.com/software/thttpd/benchmarks.html">http://www.acme.com/software/thttpd/benchmarks.html</a>
</li><li>Sun's <a href="http://jserv.javasoft.com/">Java Web Server</a>
(which has been 
<a href="http://archives.java.sun.com/cgi-bin/wa?A2=ind9901&amp;L=jserv-interest&amp;F=&amp;S=&amp;P=47739">reported to handle 500 simultaneous clients</a>)
</li></ul>

<h2><a name="examples.kio">Interesting in-kernel servers</a></h2>
<ul>
<li><a href="http://www.fenrus.demon.nl/">khttpd</a>
</li><li><a href="http://slashdot.org/comments.pl?sid=00/07/05/0211257&amp;cid=218">"TUX" (Threaded linUX webserver)</a> by Ingo Molnar et al.  For 2.4 kernel.
</li></ul>

<h2><a name="links">Other interesting links</a></h2>
<ul>
<li><a href="http://pl.atyp.us/content/tech/servers.html">Jeff Darcy's notes on high-performance server design</a>
</li><li><a href="http://www2.linuxjournal.com/lj-issues/issue91/4752.html">Ericsson's ARIES project</a> -- benchmark results for Apache 1 vs. Apache 2 vs. Tomcat
on 1 to 12 processors
</li><li><a href="http://nakula.rvs.uni-bielefeld.de/made/artikel/Web-Bench/web-bench.html">Prof. Peter Ladkin's Web Server Performance</a> page.  
</li><li><a href="http://www.novell.com/bordermanager/ispcon4.html">Novell's
FastCache</a> -- claims 10000 hits per second.  Quite the pretty performance graph.
</li><li>Rik van Riel's <a href="http://linuxperf.nl.linux.org/">Linux Performance Tuning site</a>
</li></ul>

<h2>Translations</h2>
<a href="http://ucallweconn.net/be/c10k-be">Belorussian translation</a>
provided by Patric Conrad at <a href="http://ucallweconn.net/">Ucallweconn</a>
<p>
</p><hr>
<h2>Changelog</h2>
<pre>2011/07/21
Added nginx.org

$Log: c10k.html,v $
Revision 1.212  2006/09/02 14:52:13  dank
added asio

Revision 1.211  2006/07/27 10:28:58  dank
Link to Cal Henderson's book.

Revision 1.210  2006/07/27 10:18:58  dank
Listify polyakov links, add Drepper's new proposal, note that FreeBSD 7 might move to 1:1

Revision 1.209  2006/07/13 15:07:03  dank
link to Scale! library, updated Polyakov links

Revision 1.208  2006/07/13 14:50:29  dank
Link to Polyakov's patches

Revision 1.207  2003/11/03 08:09:39  dank
Link to Linus's message deprecating the idea of aio_open

Revision 1.206  2003/11/03 07:44:34  dank
link to userver

Revision 1.205  2003/11/03 06:55:26  dank
Link to Vivek Pei's new Flash paper, mention great specweb99 score

</pre>
<hr>
<p>
<i>Copyright 1999-2018 Dan Kegel</i><br>
dank@kegel.com<br>
Last updated: 5 February 2014<br>
(minor correction: 28 May 2019)<br>
<a href="http://www.kegel.com/">[Return to www.kegel.com]</a>


</p></body></html>